{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041f865b",
   "metadata": {},
   "source": [
    "## Group D Assignment 4 Sonification of Data\n",
    "\n",
    "In this assignment we have attempted to create a piece of music by sonifying data.  We have chosen a set of weather data to sonify as it was based on occurances over time, but also was multi-dimensional enough to allow for interesting overallying of different sonified information with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0d1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import pretty_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427c6a6",
   "metadata": {},
   "source": [
    "We define the column variables which we are interested to read from csv here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc037ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "relativeHumidityColumn = \"Relative Humidity\"\n",
    "specificHumidityColumn = \"Specific Humidity\"\n",
    "temperatureColumn = \"Temperature\"\n",
    "precipitationColumn = 'Precipitation'\n",
    "yearColumn = 'Year'\n",
    "monthColumn = 'Month'\n",
    "dayColumn = 'Day'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb326d7",
   "metadata": {},
   "source": [
    "We define use arrayToMidi function to map any numpy array to midi number array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f5142d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def arrayToMidi(array):\n",
    "        midiMapping = lambda item : librosa.hz_to_midi(item)\n",
    "        return midiMapping(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff219190",
   "metadata": {},
   "source": [
    "We create EnvironmentData class in order to use later. The class keeps numpy arrays of temperature and relative humidity data as well as the multiplication of specific humidity and temperature as perceived temperature array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056bfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentData:\n",
    "    def __init__(self, csvFile):\n",
    "        self.data = pd.read_csv(csvFile)\n",
    "        self.temperatureArr = self.data[temperatureColumn].to_numpy()\n",
    "        self.relativeHumidityArr = self.data[relativeHumidityColumn].to_numpy()\n",
    "        self.yearArr = self.data[yearColumn]\n",
    "        self.perceivedTemperature()\n",
    "        \n",
    "    def perceivedTemperature(self):\n",
    "        humidityArr = self.data[specificHumidityColumn].to_numpy()\n",
    "        temperatureArr = self.data[temperatureColumn].to_numpy()\n",
    "        self.perceivedTemperatureArr = humidityArr*temperatureArr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b663c90",
   "metadata": {},
   "source": [
    "### Importing the rainfall data\n",
    "\n",
    "For our sonification, we used rainfall timeseries data from [kaggle](https://www.kaggle.com/datasets/poojag718/rainfall-timeseries-data)\n",
    "\n",
    "We mapped the temperature, humidity and perceived temperature datas to midi numbers at this step. Later we will use those data in as starting point for our sonification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab5473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = './data/Rainfall_data.csv'\n",
    "dataDf = EnvironmentData(csvFile)\n",
    "\n",
    "amountToAdd = 48\n",
    "temperatureMidis = arrayToMidi(dataDf.temperatureArr).astype(int) + amountToAdd\n",
    "relativeHumidityMidis = arrayToMidi(dataDf.relativeHumidityArr).astype(int) + amountToAdd\n",
    "perceivedTempToMidis = arrayToMidi(dataDf.perceivedTemperatureArr).astype(int) + amountToAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3d85b",
   "metadata": {},
   "source": [
    "Here we define the functions which we will use for slicing the sound. \n",
    "\n",
    "We used midi file in order to decide the duration of each slice, by taking the first note from the midi and analysing its duration by calculating the difference between start and end of the note. \"noteInSamples\" variable defines the sample each note will have.\n",
    "\n",
    "Then we used vaw files as the bases of our slicing and sliced the audio by noteInSamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf511c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliceSignal(synthFile, sr, noteInSamples):\n",
    "    synth, sr = librosa.load(synthFile, sr=sr, mono=True)   \n",
    "\n",
    "    sliceLength = round(noteInSamples) #Every note lasts for 88200 samples\n",
    "    offset = 0\n",
    "    signal_slices = np.empty((0,sliceLength)) #size of the row in samples\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            signal_slices = np.append(signal_slices, [synth[offset : offset+sliceLength]], axis = 0)\n",
    "            offset = offset + sliceLength\n",
    "        except:\n",
    "            print(\"Slicing of \" + synthFile + \"is done.\")\n",
    "            return signal_slices\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdda5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteInSamples(midiFile, multiplyFactor):\n",
    "    synthMidi = pretty_midi.PrettyMIDI(midiFile)\n",
    "    firstNoteOfSynth = synthMidi.instruments[0].notes[0]\n",
    "    start = synthMidi.time_to_tick(firstNoteOfSynth.start)\n",
    "    end = synthMidi.time_to_tick(firstNoteOfSynth.end)\n",
    "\n",
    "    noteDuration = end-start\n",
    "    noteInSeconds = synthMidi.tick_to_time(int(noteDuration))\n",
    "    noteInSamples = sr/noteInSeconds*multiplyFactor\n",
    "    return noteInSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eabb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSlicesAsVaw(path, sr, synthAsSlices, notes):\n",
    "    numberOfSlices = len(synthAsSlices[:,])\n",
    "    for i, note in enumerate(notes):\n",
    "        sf.write(path + note + '.wav', synthAsSlices[i], sr, subtype='PCM_24')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff7a96",
   "metadata": {},
   "source": [
    "### Slice the synth sounds\n",
    "\n",
    "We sliced the three synth sounds by the note range given below. After slicing we save each sliced sound which corresponds to a note in the folder \"Synth x Sliced\".\n",
    "\n",
    "Afterwards we will use those sounds for our sonification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f540ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sr=44100\n",
    "notes = ['A', 'B', 'C', 'D', 'E', 'F', 'G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53838863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing of ./Corpus/Synth 1 notes/Synth 1 Amin.wavis done.\n"
     ]
    }
   ],
   "source": [
    "noteInSamples = getNoteInSamples('./Corpus/Synth 1 notes/Synth 1 MIDI.mid', 4)\n",
    "\n",
    "#Slicing the synth 1 into 7 notes of the same duration. This is taken from Notebook 12 \n",
    "synth1AsSlices = sliceSignal('./Corpus/Synth 1 notes/Synth 1 Amin.wav', 44100, noteInSamples)\n",
    "\n",
    "slicesFolderOfSynth1 = './Corpus/Synth 1 Sliced/'\n",
    "saveSlicesAsVaw(slicesFolderOfSynth1, sr, synth1AsSlices, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f32a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing of ./Corpus/Synth 2 notes/Synth 2 Amin.wavis done.\n"
     ]
    }
   ],
   "source": [
    "noteInSamples = getNoteInSamples('./Corpus/Synth 2 notes/Synth 2 MIDI.mid', 4)\n",
    "\n",
    "#Slicing the synth 2 into 7 notes of the same duration.\n",
    "synth2AsSlices = sliceSignal('./Corpus/Synth 2 notes/Synth 2 Amin.wav', 44100, noteInSamples)\n",
    "\n",
    "slicesFolderOfSynth2 = './Corpus/Synth 2 Sliced/'\n",
    "saveSlicesAsVaw(slicesFolderOfSynth2, sr, synth2AsSlices, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d14f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing of ./Corpus/Synth 3 notes/Synth 3 Chords.wavis done.\n"
     ]
    }
   ],
   "source": [
    "noteInSamples = getNoteInSamples('./Corpus/Synth 3 notes/Synth 3 MIDI.mid', 16)\n",
    "\n",
    "#Slicing the synth 3 into 7 notes of the same duration.\n",
    "synth3AsSlices = sliceSignal('./Corpus/Synth 3 notes/Synth 3 Chords.wav', sr, noteInSamples)\n",
    "\n",
    "slicesFolderOfSynth3 = './Corpus/Synth 3 Sliced/'\n",
    "saveSlicesAsVaw(slicesFolderOfSynth3, sr, synth3AsSlices, notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386d711",
   "metadata": {},
   "source": [
    "### Sonification\n",
    "\n",
    "We used temperature, relative humidity and perceived temperature arrays from data as starting points for our sonification. Over time, we read each midi number from those arrays, define them as the pitch and play the corresponding note sound from three corpus, Synh 1, Synth 2 and Synth 3. We use numpy array values for defining the duration of each slice as well.\n",
    "\n",
    "At the end we create three musical line with three synths and add them on top of each other to finalise our sonification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c4850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def midiToAudioSlice(midiArray, durationArray, folderOfSlices, sr=48000):\n",
    "    melody = np.array([])\n",
    "    for midiNumber, duration in zip(midiArray, durationArray):\n",
    "            duration = duration / 100\n",
    "            noteName = pretty_midi.note_number_to_name(midiNumber)\n",
    "            noteWithoutDiez = noteName[0:1].replace('#', '')\n",
    "            sliceFile = folderOfSlices + noteWithoutDiez + '.wav'\n",
    "            note, sr = librosa.load(sliceFile, sr=sr, duration=duration)\n",
    "            melody = np.append(melody, note)\n",
    "    return melody   \n",
    "    \n",
    "sr=48000\n",
    "sonification = midiToAudioSlice(temperatureMidis, dataDf.temperatureArr*10, slicesFolderOfSynth1, sr)\n",
    "+ midiToAudioSlice(relativeHumidityMidis, dataDf.relativeHumidityArr*10, slicesFolderOfSynth2, sr)\n",
    "+ midiToAudioSlice(perceivedTempToMidis, dataDf.perceivedTemperatureArr, slicesFolderOfSynth3, sr)\n",
    "\n",
    "sf.write('./Sonification.wav', sonification, sr, subtype='PCM_24')\n",
    "ipd.display(ipd.Audio(sonification,rate=sr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adfc70",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "First we create a specshow of our sonification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22891a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sr = librosa.load('./Sonification.wav', sr=sr)\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "Signal_db = librosa.amplitude_to_db(np.abs(librosa.stft(signal)), ref=np.max)\n",
    "librosa.display.specshow(Signal_db, y_axis='log', fmax=16000, x_axis='time', sr=sr)\n",
    "\n",
    "plt.savefig('specshow.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0cfdf1",
   "metadata": {},
   "source": [
    "Here we tried creating an animation by using FuncAnimation but we couldnt make it work.\n",
    "\n",
    "The goal was geting noteInSamples and slicing the sonification and visualizing each slice in a frame, ordered by index as it corresponds to time as well. If we could have made it work, we could have also add year month day information to the visualisation as well.\n",
    "\n",
    "We would appreciate any feedback about how we can make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "# reshape the figure with index of time in x axis and range of amplitude in y\n",
    "ax = plt.axes(xlim=(0, len(perceivedTempToMidis)), ylim=(0, np.amax(signal)))\n",
    "temp = ax.text(1, 1, '', ha='right', va='top', fontsize=24)\n",
    "colors = plt.get_cmap('coolwarm', len(perceivedTempToMidis))\n",
    "\n",
    "# Animation function which will be called in each frame\n",
    "def animate(i):\n",
    "    # for x axis, we divide between 0 and 1 by sampling number of each frame\n",
    "    x = np.linspace(0, 1, noteInSamples)\n",
    "    # here we slice the signal by noteInSamples and get the signal for each frame\n",
    "    y = fermi(x, 0.5, [signal[i*noteInSamples : i*noteInSamples+noteInSamples]])\n",
    "    f_d.set_data(x, y)\n",
    "    f_d.set_color(colors(i))\n",
    "    temp.set_text('S')\n",
    "    temp.set_color(colors(i))\n",
    "    \n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig=fig, func=animate, frames=len(perceivedTempToMidis), interval=500, repeat=True)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d731d2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project combined elements from across the MT4001 program.  At higher level, we have made use of pandas dataframes to import our data, made extension use of librosa functions for importing and processing audio, used several functions from the pretty midi package, as well as matplotlib for visualisation.  At the lower levels we have created functions and a class to handle repeated tasks, created dictionaries for mapping our data to slices of audio and used general Python functionality to process our data into something more useable for the sonification process.\n",
    "\n",
    "A general useage and understanding of digital audio has also been used throughout when importing, slicing and resequencing audio slices as well as choosing and processing our music corpus.  Overall, we have tried to make us of all aspects of the MT4001 course in the creation of this sonification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd85ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
